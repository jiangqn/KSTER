import torch
import numpy as np

import logging
from hashlib import md5

from joeynmt.prediction import parse_test_args
from joeynmt.helpers import load_config, load_checkpoint, get_latest_checkpoint
from joeynmt.data import load_data, Dataset, make_data_iter
from joeynmt.model import build_model, _DataParallel, Model
from joeynmt.batch import Batch
from joeynmt.constants import PAD_TOKEN, BOS_TOKEN, EOS_TOKEN
from joeynmt.faiss_index import FaissIndex
from npy_append_array import NpyAppendArray

stream_handler = logging.StreamHandler()
formatter = logging.Formatter(
    "%(asctime)s %(filename)s:%(lineno)d:\n[%(levelname)s]:%(message)s",
    datefmt="%Y-%m-%d %H:%M:%S")
stream_handler.setFormatter(formatter)
logger = logging.getLogger(__name__)
logger.addHandler(stream_handler)
logger.setLevel(logging.INFO)
logger.propagate = False

def generate_id(sequence: str) -> str:
    return md5(sequence.encode()).hexdigest()

# pylint: disable=too-many-arguments,too-many-locals,no-member
def store_examples(model: Model, embedding_path: str, token_map_path: str, data: Dataset, batch_size: int,
        use_cuda: bool, level: str, n_gpu: int, batch_class: Batch, batch_type: str) \
        -> None:
    """
    Extract hidden states generated by trained model and sent them to kafka.

    :param model: model module
    :param data: dataset for validation
    :param batch_size: validation batch size
    :param batch_class: class type of batch
    :param use_cuda: if True, use CUDA
    :param level: segmentation level, one of "char", "bpe", "word"
    :param eval_metric: evaluation metric, e.g. "bleu"
    :param n_gpu: number of GPUs
    """
    assert batch_size >= n_gpu, "batch_size must be bigger than n_gpu."
    if batch_size > 1000 and batch_type == "sentence":
        logger.warning(
            "WARNING: Are you sure you meant to work on huge batches like "
            "this? 'batch_size' is > 1000 for sentence-batching. "
            "Consider decreasing it or switching to"
            " 'eval_batch_type: token'.")
    valid_iter = make_data_iter(
        dataset=data, batch_size=batch_size, batch_type=batch_type,
        shuffle=False, train=False)
    pad_index = model.src_vocab.stoi[PAD_TOKEN]
    # disable dropout
    model.eval()
    # don't track gradients during validation

    total_doc = 0
    sentence_count = 0
    disp_step = 100
    npaa = NpyAppendArray(embedding_path)
    token_map_file = open(token_map_path, "w", encoding="utf-8")
    id_st = set()

    with torch.no_grad():

        for step, valid_batch in enumerate(iter(valid_iter)):
            # run as during training to get validation loss (e.g. xent)

            batch = batch_class(valid_batch, pad_index, use_cuda=use_cuda)
            trg_length = batch.trg_length.cpu().numpy().tolist()
            trg = batch.trg.cpu().numpy()
            batch_src_texts = model.src_vocab.arrays_to_sentences(arrays=batch.src, cut_at_eos=True)
            batch_trg_texts = model.trg_vocab.arrays_to_sentences(arrays=batch.trg, cut_at_eos=False)
            sentence_count += batch.nseqs

            # sort batch now by src length and keep track of order
            sort_reverse_index = batch.sort_by_src_length()

            _, batch_hidden_states, _, _ = model._encode_decode(**vars(batch))
            batch_hidden_states = batch_hidden_states[sort_reverse_index].cpu().numpy().astype(np.float16)

            for i in range(len(batch_trg_texts)):

                src_text = " ".join(batch_src_texts[i])
                trg_tokens = batch_trg_texts[i][0: trg_length[i] - 1]
                trg_token_ids = trg[i][0: trg_length[i] - 1].tolist()
                hidden_states = batch_hidden_states[i][0: trg_length[i] - 1]
                
                sequence = src_text + BOS_TOKEN
                
                for token, token_id, embedding in zip(trg_tokens, trg_token_ids, hidden_states):
                    _id = generate_id(sequence)
                    if _id in id_st:
                        continue
                    else:
                        id_st.add(_id)
                    npaa.append(embedding[np.newaxis, :])
                    token_map_file.write(f"{token_id}\n")
                    sequence += token
                    total_doc += 1
            
            if step % disp_step == 0 and step > 0:
                logger.info(f"save {sentence_count} sentences with {total_doc} tokens")
    
    del npaa
    token_map_file.close()
    logger.info(f"save {sentence_count} sentences with {total_doc} tokens")

def build_database(cfg_file: str, ckpt: str, division: str, index_path: str, embedding_path: str, token_map_path: str, 
        batch_class: Batch = Batch, datasets: dict = None) -> None:
    """
    The function to store hidden states generated from trained transformer model. 
    Handles loading a model from checkpoint, generating hidden states by force decoding and storing them.

    :param cfg_file: path to configuration file
    :param ckpt: path to checkpoint to load
    :param batch_class: class type of batch
    :param output_path: path to output
    :param datasets: datasets to predict
    """

    logger.info("load config")
    cfg = load_config(cfg_file)
    model_dir = cfg["training"]["model_dir"]

    assert division in ["train", "dev", "test"]
    logger.info(division)

    # when checkpoint is not specified, take latest (best) from model dir
    if ckpt is None:
        ckpt = get_latest_checkpoint(model_dir)
        try:
            step = ckpt.split(model_dir+"/")[1].split(".ckpt")[0]
        except IndexError:
            step = "best"

    # load the data
    logger.info("load data")
    if datasets is None:
        train_data, dev_data, test_data, src_vocab, trg_vocab = load_data(
            data_cfg=cfg["data"], datasets=["train", "dev", "test"])
        data_to_predict = {"train": train_data, "dev": dev_data, "test": test_data}
    else:  # avoid to load data again
        data_to_predict = {"train": datasets["train"], "dev": datasets["dev"], "test": datasets["test"]}
        src_vocab = datasets["src_vocab"]
        trg_vocab = datasets["trg_vocab"]

    # parse test args
    batch_size, batch_type, use_cuda, device, n_gpu, level, eval_metric, \
        max_output_length, beam_size, beam_alpha, postprocess, \
        bpe_type, sacrebleu, decoding_description, tokenizer_info \
        = parse_test_args(cfg, mode="test")

    # load model state from disk
    logger.info("load checkpoints")
    model_checkpoint = load_checkpoint(ckpt, use_cuda=use_cuda)

    # build model and load parameters into it
    model = build_model(cfg["model"], src_vocab=src_vocab, trg_vocab=trg_vocab)
    model.load_state_dict(model_checkpoint["model_state"])

    if use_cuda:
        model.to(device)

    # multi-gpu eval
    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
        model = _DataParallel(model)

    for data_set_name, data_set in data_to_predict.items():
        if data_set is None:
            continue
        if not data_set_name == division:
            continue

        dataset_file = cfg["data"][data_set_name] + "." + cfg["data"]["trg"]
        logger.info("Force decoding on %s set (%s)..." % (data_set_name, dataset_file))
        
        logger.info("store examples")
        store_examples(model, embedding_path=embedding_path, token_map_path=token_map_path, data=data_set, 
            batch_size=batch_size, batch_class=batch_class, batch_type=batch_type, level=level,
            use_cuda=use_cuda, n_gpu=n_gpu)

        logger.info("train index")
        index = FaissIndex()
        index.train(embedding_path)
        index.add(embedding_path)
        index.export(index_path)
        del index